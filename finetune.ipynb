{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b8b83f9-496e-454e-ae1f-61e1b63eb824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (4.31.0.dev0)\n",
      "Requirement already satisfied: einops in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (0.6.1)\n",
      "Requirement already satisfied: datasets in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (2.12.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: accelerate in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (0.20.0.dev0)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: requests in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from datasets) (12.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from datasets) (2023.5.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.6.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->accelerate) (67.8.0)\n",
      "Requirement already satisfied: wheel in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->accelerate) (0.40.0)\n",
      "Requirement already satisfied: cmake in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.26.3)\n",
      "Requirement already satisfied: lit in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
      "Installing collected packages: datasets, accelerate\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.12.0\n",
      "    Uninstalling datasets-2.12.0:\n",
      "      Successfully uninstalled datasets-2.12.0\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.20.0.dev0\n",
      "    Uninstalling accelerate-0.20.0.dev0:\n",
      "      Successfully uninstalled accelerate-0.20.0.dev0\n",
      "Successfully installed accelerate-0.20.3 datasets-2.13.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers einops datasets accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaee922-7f0c-4ae4-9887-5147c3c11858",
   "metadata": {},
   "source": [
    "Import The Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1f95480-e15a-4977-b901-0a9f923b441f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/workspace/Untitled.ipynb')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification, AdamW\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Any, Dict, Tuple\n",
    "import warnings\n",
    "from threading import Event, Thread\n",
    "import textwrap\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8060673-1cda-42b8-a391-9aae62de2231",
   "metadata": {},
   "source": [
    "Mosaic Instruct Model MPT-7B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3551b84-be99-40fb-aad4-9441ba018369",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "END_KEY = \"### End\"\n",
    "INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "{response_key}\n",
    "\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    ")\n",
    "\n",
    "\n",
    "class InstructionTextGenerationPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        use_auth_token=None,\n",
    "    ) -> None:\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch_dtype,\n",
    "            trust_remote_code=trust_remote_code,\n",
    "            use_auth_token=use_auth_token,\n",
    "        )\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=trust_remote_code,\n",
    "            use_auth_token=use_auth_token,\n",
    "        )\n",
    "        if tokenizer.pad_token_id is None:\n",
    "            warnings.warn(\n",
    "                \"pad_token_id is not set for the tokenizer. Using eos_token_id as pad_token_id.\"\n",
    "            )\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.padding_side = \"right\" # \"left\"\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.eval()\n",
    "        self.model.to(device=device, dtype=torch_dtype)\n",
    "\n",
    "        self.generate_kwargs = {\n",
    "            \"temperature\": 0.5,\n",
    "            \"top_p\": 0.92,\n",
    "            \"top_k\": 0,\n",
    "            \"max_new_tokens\": 512,\n",
    "            \"use_cache\": True,\n",
    "            \"do_sample\": True,\n",
    "            \"eos_token_id\": self.tokenizer.eos_token_id,\n",
    "            \"pad_token_id\": self.tokenizer.pad_token_id,\n",
    "            \"repetition_penalty\": 1.1,  # 1.0 means no penalty, > 1.0 means penalty, 1.2 from CTRL paper\n",
    "        }\n",
    "\n",
    "    def format_instruction(self, instruction):\n",
    "        return PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction)\n",
    "\n",
    "    def __call__(\n",
    "        self, instruction: str, **generate_kwargs: Dict[str, Any]\n",
    "    ) -> Tuple[str, str, float]:\n",
    "        s = PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction)\n",
    "        input_ids = self.tokenizer(s, return_tensors=\"pt\").input_ids\n",
    "        input_ids = input_ids.to(self.model.device)\n",
    "        gkw = {**self.generate_kwargs, **generate_kwargs}\n",
    "        with torch.no_grad():\n",
    "            output_ids = self.model.generate(input_ids, **gkw)\n",
    "        # Slice the output_ids tensor to get only new tokens\n",
    "        new_tokens = output_ids[0, len(input_ids[0]) :]\n",
    "        output_text = self.tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "        return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8287b89-fc45-4152-a56f-4f8f7a4bcb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "477ec8020b9a46d58af5e717c8406844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86521ffb468949649d4c48c0ddbefea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)configuration_mpt.py:   0%|          | 0.00/9.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b-instruct:\n",
      "- configuration_mpt.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4151d03e05ec43d9af0c56558927b84f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)main/modeling_mpt.py:   0%|          | 0.00/19.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a5b8749c1e4002ba70f8db73c52a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)meta_init_context.py:   0%|          | 0.00/3.64k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b-instruct:\n",
      "- meta_init_context.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0138ffec1a4f279c17c72a3314f94e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/custom_embedding.py:   0%|          | 0.00/305 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b-instruct:\n",
      "- custom_embedding.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e751cd94414b4fc082ddf42785b0f309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)in/param_init_fns.py:   0%|          | 0.00/12.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e7e314c15246b083018789119d34b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)resolve/main/norm.py:   0%|          | 0.00/2.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b-instruct:\n",
      "- norm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b-instruct:\n",
      "- param_init_fns.py\n",
      "- norm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769dbe06fc624f4b94660ce300bd68dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)refixlm_converter.py:   0%|          | 0.00/27.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b-instruct:\n",
      "- hf_prefixlm_converter.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b4265facf946c09ea773f34e4f1a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/blocks.py:   0%|          | 0.00/2.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a39e9631b142659168d12c65ebfbf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/attention.py:   0%|          | 0.00/17.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d474102e551e415988f6ce8e5c3a81df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)flash_attn_triton.py:   0%|          | 0.00/28.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b-instruct:\n",
      "- flash_attn_triton.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b-instruct:\n",
      "- attention.py\n",
      "- flash_attn_triton.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b-instruct:\n",
      "- blocks.py\n",
      "- attention.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337cefda46e1455caf306445ab48d07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)n/adapt_tokenizer.py:   0%|          | 0.00/1.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b-instruct:\n",
      "- adapt_tokenizer.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b-instruct:\n",
      "- modeling_mpt.py\n",
      "- meta_init_context.py\n",
      "- custom_embedding.py\n",
      "- param_init_fns.py\n",
      "- hf_prefixlm_converter.py\n",
      "- blocks.py\n",
      "- adapt_tokenizer.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8677f502fa18459b8b1bc9333f5094ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/16.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33df934c582946a9ab5f432bf7d17ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7fcf4be2065461688fe30ff6e33506e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5df28ccab446ef8c6ffe1cedff34e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/3.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating an MPTForCausalLM model from /root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-instruct/1fc4634127ec64a45716003578b9cfae23265849/modeling_mpt.py\n",
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e75fb488cd42a29d99c2df1ffe1da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f034236a8a14b68b4777025285d6d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/91.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a33be60aac3d41e6b2b6fb1a7524e5f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3cbe8bee0364331b2348537dc4ec37e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d866868caabb481c95a21d29b251e181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2808/1407196826.py:38: UserWarning: pad_token_id is not set for the tokenizer. Using eos_token_id as pad_token_id.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "generate = InstructionTextGenerationPipeline(\n",
    "    \"mosaicml/mpt-7b-instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "stop_token_ids = generate.tokenizer.convert_tokens_to_ids([\"<|endoftext|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e2cb7c1-8a54-4475-9ed0-426179b928e7",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define a custom stopping criteria\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_id in stop_token_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def process_stream(instruction, temperature, top_p, top_k, max_new_tokens):\n",
    "    # Tokenize the input\n",
    "    input_ids = generate.tokenizer(\n",
    "        generate.format_instruction(instruction), return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "    input_ids = input_ids.to(generate.model.device)\n",
    "\n",
    "    # Initialize the streamer and stopping criteria\n",
    "    streamer = TextIteratorStreamer(\n",
    "        generate.tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True\n",
    "    )\n",
    "    stop = StopOnTokens()\n",
    "\n",
    "    if temperature < 0.1:\n",
    "        temperature = 0.0\n",
    "        do_sample = False\n",
    "    else:\n",
    "        do_sample = True\n",
    "\n",
    "    gkw = {\n",
    "        **generate.generate_kwargs,\n",
    "        **{\n",
    "            \"input_ids\": input_ids,\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"do_sample\": do_sample,\n",
    "            \"top_p\": top_p,\n",
    "            \"top_k\": top_k,\n",
    "            \"streamer\": streamer,\n",
    "            \"stopping_criteria\": StoppingCriteriaList([stop]),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    response = ''\n",
    "\n",
    "    def generate_and_signal_complete():\n",
    "        generate.model.generate(**gkw)\n",
    "\n",
    "    t1 = Thread(target=generate_and_signal_complete)\n",
    "    t1.start()\n",
    "\n",
    "    for new_text in streamer:\n",
    "        response += new_text\n",
    "   \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d2fe721-ae1c-49f8-9d3a-84e468a3b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(instruction, temperature = 0.3, top_p = 0.95, top_k = 0, max_new_tokens = 2000):\n",
    "  response = process_stream(instruction, temperature, top_p, top_k, max_new_tokens)\n",
    "\n",
    "  wrapped_text = textwrap.fill(response, width=100)\n",
    "  print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d747e3f0-b68a-4be1-ab97-f1a58d79deaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n"
     ]
    }
   ],
   "source": [
    "generate_text('Hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f20b49-ee84-4615-8db4-0e508b24736d",
   "metadata": {},
   "source": [
    "Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04904bda-ed0b-452a-b8eb-f39d628a5992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2681d0e58fc44fd0a29dd78fe0a29c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32245c97aa8c46f09241de5df0015e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ed8b6e8d5c451580af0c01da835bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset ag_news/default to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a74f579c51be486fb35be82a1d81c08a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/11.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e669b5c62e421a8422f282ca2ce44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/751k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ad1c682d9d4c148f3cd1015ee96ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['World', 'Sports', 'Business', 'Sci/Tech']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('ag_news')\n",
    "# Extract label names\n",
    "label_names = dataset['train'].features['label'].names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b11cfd4a-6f31-4390-b753-22335e083b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function <lambda> at 0x7f8f3fee70d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_names_str = str(label_names).replace(\"'\",\"\")\n",
    "prefix = f\"Classify the following text into one of the following categories: {label_names_str}\\nText: \"\n",
    "\n",
    "num_samples_per_class = 75  # Set this to determine the number of samples per class\n",
    "\n",
    "# Extract subsets of the training dataset with equal number of samples per class\n",
    "train_datasets = []\n",
    "\n",
    "for label in range(len(label_names)):\n",
    "    label_dataset = dataset['train'].filter(lambda x: x['label'] == label).select(range(num_samples_per_class))\n",
    "    label_dataset = label_dataset.map(lambda x: {'text': generate.format_instruction(prefix + x['text']) + label_names[x['label']]})\n",
    "    train_datasets.append(label_dataset)\n",
    "\n",
    "train_dataset = concatenate_datasets(train_datasets)\n",
    "\n",
    "# Do the same for the validation dataset, if needed\n",
    "val_datasets = []\n",
    "for label in range(len(label_names)):\n",
    "    label_dataset = dataset['test'].filter(lambda x: x['label'] == label).select(range(num_samples_per_class))\n",
    "    label_dataset = label_dataset.map(lambda x: {'text': generate.format_instruction(prefix + x['text']) + label_names[x['label']]})\n",
    "    val_datasets.append(label_dataset)\n",
    "\n",
    "val_dataset = concatenate_datasets(val_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "967e06b7-e17d-4eec-8035-b57b76566bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the datasets\n",
    "train_encodings = generate.tokenizer(train_dataset['text'], truncation=True, padding=True, max_length=256)\n",
    "val_encodings = generate.tokenizer(val_dataset['text'], truncation=True, padding=True, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67f74cba-8e94-4336-822e-469c4256dc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the encodings to PyTorch datasets\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = item[\"input_ids\"].clone()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee02d7bd-065c-4a88-87a1-b14154efdc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the encodings to PyTorch datasets\n",
    "train_dataset = TextDataset(train_encodings)\n",
    "val_dataset = TextDataset(val_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2fae56-e12f-426a-a02d-2ea344492cd5",
   "metadata": {},
   "source": [
    "Before Finetuning, Let's see the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f844a675-9665-40be-b086-85240265e4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example text:\n",
      "Classify the following text into one of the following categories: [World, Sports, Business,\n",
      "Sci/Tech] Text: Fears for T N pension after talks Unions representing workers at Turner   Newall say\n",
      "they are 'disappointed' after talks with stricken parent firm Federal Mogul.\n",
      "Response:\n",
      "The above paragraph can be classified as follows; World - Disappointment from unions over failed\n",
      "negotiations between their employer and its owner (Turner Newall).\n",
      "Correct Label:\n",
      "Business\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "example_text = prefix + dataset['test'][index]['text'] \n",
    "print('Example text:')\n",
    "print(textwrap.fill(example_text, width=100))\n",
    "print(\"Response:\")\n",
    "generate_text(example_text, temperature=0)\n",
    "print(\"Correct Label:\")\n",
    "print(label_names[dataset['test'][index]['label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35798e9-5526-4991-9209-5d6e5d6379d9",
   "metadata": {},
   "source": [
    "Freeze the Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea7db49f-ed24-4459-b7bf-5aa9c6f2e397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight   Modelsize: 206.6M parameters\n",
      "transformer.wte.weight False\n",
      "transformer.blocks.0.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.0.norm_1.weight False\n",
      "transformer.blocks.0.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.0.attn.Wqkv.weight False\n",
      "transformer.blocks.0.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.0.attn.out_proj.weight False\n",
      "transformer.blocks.0.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.0.norm_2.weight False\n",
      "transformer.blocks.0.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.0.ffn.up_proj.weight False\n",
      "transformer.blocks.0.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.0.ffn.down_proj.weight False\n",
      "transformer.blocks.1.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.1.norm_1.weight False\n",
      "transformer.blocks.1.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.1.attn.Wqkv.weight False\n",
      "transformer.blocks.1.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.1.attn.out_proj.weight False\n",
      "transformer.blocks.1.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.1.norm_2.weight False\n",
      "transformer.blocks.1.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.1.ffn.up_proj.weight False\n",
      "transformer.blocks.1.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.1.ffn.down_proj.weight False\n",
      "transformer.blocks.2.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.2.norm_1.weight False\n",
      "transformer.blocks.2.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.2.attn.Wqkv.weight False\n",
      "transformer.blocks.2.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.2.attn.out_proj.weight False\n",
      "transformer.blocks.2.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.2.norm_2.weight False\n",
      "transformer.blocks.2.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.2.ffn.up_proj.weight False\n",
      "transformer.blocks.2.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.2.ffn.down_proj.weight False\n",
      "transformer.blocks.3.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.3.norm_1.weight False\n",
      "transformer.blocks.3.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.3.attn.Wqkv.weight False\n",
      "transformer.blocks.3.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.3.attn.out_proj.weight False\n",
      "transformer.blocks.3.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.3.norm_2.weight False\n",
      "transformer.blocks.3.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.3.ffn.up_proj.weight False\n",
      "transformer.blocks.3.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.3.ffn.down_proj.weight False\n",
      "transformer.blocks.4.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.4.norm_1.weight False\n",
      "transformer.blocks.4.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.4.attn.Wqkv.weight False\n",
      "transformer.blocks.4.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.4.attn.out_proj.weight False\n",
      "transformer.blocks.4.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.4.norm_2.weight False\n",
      "transformer.blocks.4.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.4.ffn.up_proj.weight False\n",
      "transformer.blocks.4.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.4.ffn.down_proj.weight False\n",
      "transformer.blocks.5.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.5.norm_1.weight False\n",
      "transformer.blocks.5.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.5.attn.Wqkv.weight False\n",
      "transformer.blocks.5.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.5.attn.out_proj.weight False\n",
      "transformer.blocks.5.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.5.norm_2.weight False\n",
      "transformer.blocks.5.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.5.ffn.up_proj.weight False\n",
      "transformer.blocks.5.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.5.ffn.down_proj.weight False\n",
      "transformer.blocks.6.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.6.norm_1.weight False\n",
      "transformer.blocks.6.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.6.attn.Wqkv.weight False\n",
      "transformer.blocks.6.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.6.attn.out_proj.weight False\n",
      "transformer.blocks.6.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.6.norm_2.weight False\n",
      "transformer.blocks.6.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.6.ffn.up_proj.weight False\n",
      "transformer.blocks.6.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.6.ffn.down_proj.weight False\n",
      "transformer.blocks.7.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.7.norm_1.weight False\n",
      "transformer.blocks.7.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.7.attn.Wqkv.weight False\n",
      "transformer.blocks.7.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.7.attn.out_proj.weight False\n",
      "transformer.blocks.7.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.7.norm_2.weight False\n",
      "transformer.blocks.7.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.7.ffn.up_proj.weight False\n",
      "transformer.blocks.7.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.7.ffn.down_proj.weight False\n",
      "transformer.blocks.8.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.8.norm_1.weight False\n",
      "transformer.blocks.8.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.8.attn.Wqkv.weight False\n",
      "transformer.blocks.8.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.8.attn.out_proj.weight False\n",
      "transformer.blocks.8.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.8.norm_2.weight False\n",
      "transformer.blocks.8.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.8.ffn.up_proj.weight False\n",
      "transformer.blocks.8.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.8.ffn.down_proj.weight False\n",
      "transformer.blocks.9.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.9.norm_1.weight False\n",
      "transformer.blocks.9.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.9.attn.Wqkv.weight False\n",
      "transformer.blocks.9.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.9.attn.out_proj.weight False\n",
      "transformer.blocks.9.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.9.norm_2.weight False\n",
      "transformer.blocks.9.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.9.ffn.up_proj.weight False\n",
      "transformer.blocks.9.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.9.ffn.down_proj.weight False\n",
      "transformer.blocks.10.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.10.norm_1.weight False\n",
      "transformer.blocks.10.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.10.attn.Wqkv.weight False\n",
      "transformer.blocks.10.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.10.attn.out_proj.weight False\n",
      "transformer.blocks.10.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.10.norm_2.weight False\n",
      "transformer.blocks.10.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.10.ffn.up_proj.weight False\n",
      "transformer.blocks.10.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.10.ffn.down_proj.weight False\n",
      "transformer.blocks.11.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.11.norm_1.weight False\n",
      "transformer.blocks.11.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.11.attn.Wqkv.weight False\n",
      "transformer.blocks.11.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.11.attn.out_proj.weight False\n",
      "transformer.blocks.11.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.11.norm_2.weight False\n",
      "transformer.blocks.11.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.11.ffn.up_proj.weight False\n",
      "transformer.blocks.11.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.11.ffn.down_proj.weight False\n",
      "transformer.blocks.12.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.12.norm_1.weight False\n",
      "transformer.blocks.12.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.12.attn.Wqkv.weight False\n",
      "transformer.blocks.12.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.12.attn.out_proj.weight False\n",
      "transformer.blocks.12.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.12.norm_2.weight False\n",
      "transformer.blocks.12.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.12.ffn.up_proj.weight False\n",
      "transformer.blocks.12.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.12.ffn.down_proj.weight False\n",
      "transformer.blocks.13.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.13.norm_1.weight False\n",
      "transformer.blocks.13.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.13.attn.Wqkv.weight False\n",
      "transformer.blocks.13.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.13.attn.out_proj.weight False\n",
      "transformer.blocks.13.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.13.norm_2.weight False\n",
      "transformer.blocks.13.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.13.ffn.up_proj.weight False\n",
      "transformer.blocks.13.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.13.ffn.down_proj.weight False\n",
      "transformer.blocks.14.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.14.norm_1.weight False\n",
      "transformer.blocks.14.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.14.attn.Wqkv.weight False\n",
      "transformer.blocks.14.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.14.attn.out_proj.weight False\n",
      "transformer.blocks.14.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.14.norm_2.weight False\n",
      "transformer.blocks.14.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.14.ffn.up_proj.weight False\n",
      "transformer.blocks.14.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.14.ffn.down_proj.weight False\n",
      "transformer.blocks.15.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.15.norm_1.weight False\n",
      "transformer.blocks.15.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.15.attn.Wqkv.weight False\n",
      "transformer.blocks.15.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.15.attn.out_proj.weight False\n",
      "transformer.blocks.15.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.15.norm_2.weight False\n",
      "transformer.blocks.15.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.15.ffn.up_proj.weight False\n",
      "transformer.blocks.15.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.15.ffn.down_proj.weight False\n",
      "transformer.blocks.16.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.16.norm_1.weight False\n",
      "transformer.blocks.16.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.16.attn.Wqkv.weight False\n",
      "transformer.blocks.16.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.16.attn.out_proj.weight False\n",
      "transformer.blocks.16.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.16.norm_2.weight False\n",
      "transformer.blocks.16.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.16.ffn.up_proj.weight False\n",
      "transformer.blocks.16.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.16.ffn.down_proj.weight False\n",
      "transformer.blocks.17.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.17.norm_1.weight False\n",
      "transformer.blocks.17.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.17.attn.Wqkv.weight False\n",
      "transformer.blocks.17.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.17.attn.out_proj.weight False\n",
      "transformer.blocks.17.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.17.norm_2.weight False\n",
      "transformer.blocks.17.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.17.ffn.up_proj.weight False\n",
      "transformer.blocks.17.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.17.ffn.down_proj.weight False\n",
      "transformer.blocks.18.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.18.norm_1.weight False\n",
      "transformer.blocks.18.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.18.attn.Wqkv.weight False\n",
      "transformer.blocks.18.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.18.attn.out_proj.weight False\n",
      "transformer.blocks.18.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.18.norm_2.weight False\n",
      "transformer.blocks.18.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.18.ffn.up_proj.weight False\n",
      "transformer.blocks.18.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.18.ffn.down_proj.weight False\n",
      "transformer.blocks.19.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.19.norm_1.weight False\n",
      "transformer.blocks.19.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.19.attn.Wqkv.weight False\n",
      "transformer.blocks.19.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.19.attn.out_proj.weight False\n",
      "transformer.blocks.19.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.19.norm_2.weight False\n",
      "transformer.blocks.19.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.19.ffn.up_proj.weight False\n",
      "transformer.blocks.19.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.19.ffn.down_proj.weight False\n",
      "transformer.blocks.20.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.20.norm_1.weight False\n",
      "transformer.blocks.20.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.20.attn.Wqkv.weight False\n",
      "transformer.blocks.20.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.20.attn.out_proj.weight False\n",
      "transformer.blocks.20.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.20.norm_2.weight False\n",
      "transformer.blocks.20.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.20.ffn.up_proj.weight False\n",
      "transformer.blocks.20.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.20.ffn.down_proj.weight False\n",
      "transformer.blocks.21.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.21.norm_1.weight False\n",
      "transformer.blocks.21.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.21.attn.Wqkv.weight False\n",
      "transformer.blocks.21.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.21.attn.out_proj.weight False\n",
      "transformer.blocks.21.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.21.norm_2.weight False\n",
      "transformer.blocks.21.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.21.ffn.up_proj.weight False\n",
      "transformer.blocks.21.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.21.ffn.down_proj.weight False\n",
      "transformer.blocks.22.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.22.norm_1.weight False\n",
      "transformer.blocks.22.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.22.attn.Wqkv.weight False\n",
      "transformer.blocks.22.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.22.attn.out_proj.weight False\n",
      "transformer.blocks.22.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.22.norm_2.weight False\n",
      "transformer.blocks.22.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.22.ffn.up_proj.weight False\n",
      "transformer.blocks.22.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.22.ffn.down_proj.weight False\n",
      "transformer.blocks.23.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.23.norm_1.weight False\n",
      "transformer.blocks.23.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.23.attn.Wqkv.weight False\n",
      "transformer.blocks.23.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.23.attn.out_proj.weight False\n",
      "transformer.blocks.23.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.23.norm_2.weight False\n",
      "transformer.blocks.23.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.23.ffn.up_proj.weight False\n",
      "transformer.blocks.23.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.23.ffn.down_proj.weight False\n",
      "transformer.blocks.24.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.24.norm_1.weight False\n",
      "transformer.blocks.24.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.24.attn.Wqkv.weight False\n",
      "transformer.blocks.24.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.24.attn.out_proj.weight False\n",
      "transformer.blocks.24.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.24.norm_2.weight False\n",
      "transformer.blocks.24.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.24.ffn.up_proj.weight False\n",
      "transformer.blocks.24.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.24.ffn.down_proj.weight False\n",
      "transformer.blocks.25.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.25.norm_1.weight False\n",
      "transformer.blocks.25.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.25.attn.Wqkv.weight False\n",
      "transformer.blocks.25.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.25.attn.out_proj.weight False\n",
      "transformer.blocks.25.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.25.norm_2.weight False\n",
      "transformer.blocks.25.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.25.ffn.up_proj.weight False\n",
      "transformer.blocks.25.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.25.ffn.down_proj.weight False\n",
      "transformer.blocks.26.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.26.norm_1.weight False\n",
      "transformer.blocks.26.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.26.attn.Wqkv.weight False\n",
      "transformer.blocks.26.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.26.attn.out_proj.weight False\n",
      "transformer.blocks.26.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.26.norm_2.weight False\n",
      "transformer.blocks.26.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.26.ffn.up_proj.weight False\n",
      "transformer.blocks.26.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.26.ffn.down_proj.weight False\n",
      "transformer.blocks.27.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.27.norm_1.weight False\n",
      "transformer.blocks.27.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.27.attn.Wqkv.weight False\n",
      "transformer.blocks.27.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.27.attn.out_proj.weight False\n",
      "transformer.blocks.27.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.27.norm_2.weight False\n",
      "transformer.blocks.27.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.27.ffn.up_proj.weight False\n",
      "transformer.blocks.27.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.27.ffn.down_proj.weight False\n",
      "transformer.blocks.28.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.28.norm_1.weight False\n",
      "transformer.blocks.28.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.28.attn.Wqkv.weight False\n",
      "transformer.blocks.28.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.28.attn.out_proj.weight False\n",
      "transformer.blocks.28.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.28.norm_2.weight False\n",
      "transformer.blocks.28.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.28.ffn.up_proj.weight False\n",
      "transformer.blocks.28.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.28.ffn.down_proj.weight False\n",
      "transformer.blocks.29.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.29.norm_1.weight False\n",
      "transformer.blocks.29.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.29.attn.Wqkv.weight False\n",
      "transformer.blocks.29.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.29.attn.out_proj.weight False\n",
      "transformer.blocks.29.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.29.norm_2.weight False\n",
      "transformer.blocks.29.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.29.ffn.up_proj.weight False\n",
      "transformer.blocks.29.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.29.ffn.down_proj.weight False\n",
      "transformer.blocks.30.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.30.norm_1.weight False\n",
      "transformer.blocks.30.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.30.attn.Wqkv.weight False\n",
      "transformer.blocks.30.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.30.attn.out_proj.weight False\n",
      "transformer.blocks.30.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.30.norm_2.weight False\n",
      "transformer.blocks.30.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.30.ffn.up_proj.weight False\n",
      "transformer.blocks.30.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.30.ffn.down_proj.weight False\n",
      "transformer.blocks.31.norm_1.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.31.norm_1.weight True\n",
      "transformer.blocks.31.attn.Wqkv.weight   Modelsize: 50.3M parameters\n",
      "transformer.blocks.31.attn.Wqkv.weight True\n",
      "transformer.blocks.31.attn.out_proj.weight   Modelsize: 16.8M parameters\n",
      "transformer.blocks.31.attn.out_proj.weight True\n",
      "transformer.blocks.31.norm_2.weight   Modelsize: 0.0M parameters\n",
      "transformer.blocks.31.norm_2.weight True\n",
      "transformer.blocks.31.ffn.up_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.31.ffn.up_proj.weight True\n",
      "transformer.blocks.31.ffn.down_proj.weight   Modelsize: 67.1M parameters\n",
      "transformer.blocks.31.ffn.down_proj.weight True\n",
      "transformer.norm_f.weight   Modelsize: 0.0M parameters\n",
      "transformer.norm_f.weight False\n"
     ]
    }
   ],
   "source": [
    "for name, param in generate.model.named_parameters():\n",
    "    print(f\"{name}   Modelsize: {param.numel()/1000**2:.1f}M parameters\")\n",
    "    if \"31\" not in name:\n",
    "        param.requires_grad = False\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43210068-d48a-450b-bf3d-a2eddbf36a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    num_train_epochs=50,              \n",
    "    per_device_train_batch_size=16,  \n",
    "    per_device_eval_batch_size=64,   \n",
    "    warmup_steps=500,                \n",
    "    weight_decay=0.01,              \n",
    "    logging_dir='./logs',            \n",
    "    logging_steps=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0361561-228e-4f36-9e96-e4a843222451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Trainer and train\n",
    "trainer = Trainer(\n",
    "    model=generate.model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=val_dataset             \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4df0b1e-7f9c-4556-af9e-b123f7322ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20230708_131846-1arkrdte</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/prompt-engineer48/huggingface/runs/1arkrdte' target=\"_blank\">glad-jazz-1</a></strong> to <a href='https://wandb.ai/prompt-engineer48/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/prompt-engineer48/huggingface' target=\"_blank\">https://wandb.ai/prompt-engineer48/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/prompt-engineer48/huggingface/runs/1arkrdte' target=\"_blank\">https://wandb.ai/prompt-engineer48/huggingface/runs/1arkrdte</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='950' max='950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [950/950 10:10, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.032800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.946900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.901600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.634400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.268700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.103100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.899200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.684400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.483600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.363300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.221100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.104300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.025800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.028900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.035500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.986700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.989800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.950400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.989800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.979700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.909400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.947700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.923800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.927000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.896500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.905500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.943400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.853500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.908600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.872700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.863300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.839800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.911700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.832000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.855100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.805500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.827700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.820700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.802300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.832000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.764800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.796100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.714100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.775400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.753900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.743400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.728500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.716800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.729300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.705900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.701200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.687900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.675800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.678100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.655100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.665200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.659400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.662900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.617600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.671900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.625800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.651200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.613300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.628900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.611300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.591400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.628500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.631200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.586300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.613700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.604300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.610200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.599600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.607400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.598000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.582000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.611300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.590200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.607800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.582800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.578100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.610500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.592800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.595900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.600800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.597100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.579700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.608200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.602300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.589500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.568700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.609400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=950, training_loss=0.9295086348684211, metrics={'train_runtime': 677.3493, 'train_samples_per_second': 22.145, 'train_steps_per_second': 1.403, 'total_flos': 9.509450563584e+16, 'train_loss': 0.9295086348684211, 'epoch': 50.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aaaa9adc-3e3d-48b4-94ee-d1f032c7e9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPTForCausalLM(\n",
       "  (transformer): MPTModel(\n",
       "    (wte): SharedEmbedding(50432, 4096)\n",
       "    (emb_drop): Dropout(p=0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm_f): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate.model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5fdd49-0fb4-4b70-b4cd-92767dcd697a",
   "metadata": {},
   "source": [
    "LETS TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1c2af83-44fd-4cb2-ba12-60a9632e1313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example text:\n",
      "Classify the following text into one of the following categories: [World, Sports, Business,\n",
      "Sci/Tech] Text: Calif. Aims to Limit Farm-Related Smog (AP) AP - Southern California's smog-fighting\n",
      "agency went after emissions of the bovine variety Friday, adopting the nation's first rules to\n",
      "reduce air pollution from dairy cow manure.\n",
      "Response:\n",
      "Sci/Tech\n",
      "Correct Label:\n",
      "Sci/Tech\n"
     ]
    }
   ],
   "source": [
    "index = 4\n",
    "example_text = prefix + dataset['test'][index]['text'] \n",
    "print('Example text:')\n",
    "print(textwrap.fill(example_text, width=100))\n",
    "print(\"Response:\")\n",
    "generate_text(example_text, temperature=0)\n",
    "print(\"Correct Label:\")\n",
    "print(label_names[dataset['test'][index]['label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6acb5e-5622-4844-9d7b-54dcf4aeaa2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
